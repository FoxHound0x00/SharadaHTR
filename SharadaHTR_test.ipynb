{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/FoxHound0x00/SharadaHTR/blob/test/SharadaHTR_test.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RMoJI7tbB4TW",
        "outputId": "0e445eae-e287-4e88-b7cf-2cd0950e6ca6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: imageio in /usr/local/lib/python3.10/dist-packages (2.31.6)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (3.7.1)\n",
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.10/dist-packages (4.8.0.76)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (1.5.3)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.10/dist-packages (9.4.0)\n",
            "Requirement already satisfied: python-Levenshtein in /usr/local/lib/python3.10/dist-packages (0.23.0)\n",
            "Requirement already satisfied: scikit-image in /usr/local/lib/python3.10/dist-packages (0.19.3)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (1.11.4)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.1.0+cu121)\n",
            "Requirement already satisfied: torchaudio in /usr/local/lib/python3.10/dist-packages (2.1.0+cu121)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (0.16.0+cu121)\n",
            "Collecting torchinfo\n",
            "  Downloading torchinfo-1.8.0-py3-none-any.whl (23 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from imageio) (1.23.5)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.2.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (4.47.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.4.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (23.2)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (3.1.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2023.3.post1)\n",
            "Requirement already satisfied: Levenshtein==0.23.0 in /usr/local/lib/python3.10/dist-packages (from python-Levenshtein) (0.23.0)\n",
            "Requirement already satisfied: rapidfuzz<4.0.0,>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from Levenshtein==0.23.0->python-Levenshtein) (3.6.1)\n",
            "Requirement already satisfied: networkx>=2.2 in /usr/local/lib/python3.10/dist-packages (from scikit-image) (3.2.1)\n",
            "Requirement already satisfied: tifffile>=2019.7.26 in /usr/local/lib/python3.10/dist-packages (from scikit-image) (2023.12.9)\n",
            "Requirement already satisfied: PyWavelets>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-image) (1.5.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.13.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch) (4.5.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.12)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.2)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2023.6.0)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch) (2.1.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torchvision) (2.31.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (2.1.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision) (2023.11.17)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n",
            "Installing collected packages: torchinfo\n",
            "Successfully installed torchinfo-1.8.0\n"
          ]
        }
      ],
      "source": [
        "!pip3 install imageio matplotlib opencv-python pandas pillow python-Levenshtein scikit-image scipy torch torchaudio torchvision torchinfo"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aL26oo5xB8vn",
        "outputId": "debf5bc5-9ff8-4792-ff46-3ae5cab7e181"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!cp -r /content/drive/MyDrive/Sharada_files/output.tar.gz /content/\n",
        "!tar -xf /content/output.tar.gz"
      ],
      "metadata": {
        "id": "sPeBFRDJCDng"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print('Using device:', device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5p1QQfRGCDtQ",
        "outputId": "00390bdf-106c-472d-aaf6-8f77bbf7e123"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import shutil\n",
        "import re\n",
        "\n",
        "DIR = \"output/\"\n",
        "DST_DIR = \"temp_annot/\"\n",
        "\n",
        "os.makedirs(DST_DIR,exist_ok=True)\n",
        "files = os.listdir(DIR)\n",
        "\n",
        "\n",
        "for f in files:\n",
        "    if f.endswith('.json'):\n",
        "        json_file = os.path.join(DIR,f)\n",
        "        img_file = os.path.join(DIR,os.path.splitext(f)[0]+'.png')\n",
        "        if os.path.isfile(img_file):\n",
        "            shutil.copy(img_file,DST_DIR)\n",
        "            shutil.copy(json_file,DST_DIR)"
      ],
      "metadata": {
        "id": "ZxIHcspHCQRf"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import random\n",
        "import string\n",
        "import numpy as np\n",
        "from PIL import Image, ImageDraw, ImageOps\n",
        "\n",
        "os.makedirs(\"temp_annot/\",exist_ok=True)\n",
        "os.makedirs(\"extracted_dir/\",exist_ok=True)\n",
        "\n",
        "src_dir =  \"temp_annot/\"\n",
        "dest_dir = \"extracted_dir/\"\n",
        "\n",
        "for file in os.listdir(src_dir):\n",
        "    if not file.endswith('.json'):\n",
        "\n",
        "        img_path = os.path.join(src_dir,file)\n",
        "        json_path = os.path.join(src_dir,os.path.splitext(file)[0]+'.json')\n",
        "\n",
        "        if os.path.exists(json_path):\n",
        "            with open(json_path, 'r', encoding='utf-8') as f:\n",
        "                annotation = json.load(f)\n",
        "\n",
        "            image_height = annotation['imageHeight']\n",
        "            image_width = annotation['imageWidth']\n",
        "            image = Image.open(img_path)\n",
        "\n",
        "\n",
        "            for shape in annotation['shapes']:\n",
        "                shape_type = shape['shape_type']\n",
        "                group_id = shape['group_id']\n",
        "                label = shape['label']\n",
        "                coordinates = shape['points']\n",
        "                cropped_image = Image.new('RGBA', (image_width, image_height), (0, 0, 0, 0))\n",
        "                draw = ImageDraw.Draw(cropped_image)\n",
        "                random_string = ''.join(random.choices(string.ascii_letters + string.digits, k=15))\n",
        "                int_coordinates = [(int(point[0]), int(point[1])) for point in coordinates]\n",
        "                draw.polygon(int_coordinates, fill=(255, 255, 255, 255))\n",
        "\n",
        "\n",
        "                if shape_type == 'polygon':\n",
        "                    mask = ImageOps.invert(cropped_image.convert('L'))\n",
        "\n",
        "                    masked_image = Image.new(\"1\", image.size)\n",
        "                    masked_image.paste(image, (0, 0), mask=cropped_image)\n",
        "                    bbox = masked_image.getbbox()\n",
        "\n",
        "                    if bbox:\n",
        "                        cropped_image = masked_image.crop(bbox)\n",
        "                        rectangular_image = Image.new(\"1\", (cropped_image.width, cropped_image.height), (255, 255, 255))\n",
        "                        rectangular_image.paste(cropped_image, (0, 0), cropped_image)\n",
        "                        new_ImageName = os.path.join(dest_dir, f\"{random_string}.jpg\")\n",
        "                        new_LabelName = os.path.join(dest_dir, f\"{random_string}.txt\")\n",
        "                        rectangular_image.save(new_ImageName, format='JPEG', quality=100)\n",
        "                        open(new_LabelName, \"w\", encoding=\"utf-8\").write(label)\n",
        "\n",
        "                if shape_type == 'rectangle':\n",
        "                    x_coordinates = [point[0] for point in int_coordinates]\n",
        "                    y_coordinates = [point[1] for point in int_coordinates]\n",
        "                    left = min(x_coordinates)\n",
        "                    top = min(y_coordinates)\n",
        "                    right = max(x_coordinates)\n",
        "                    bottom = max(y_coordinates)\n",
        "                    cropped_image = image.crop((left, top, right, bottom))\n",
        "                    new_ImageName = os.path.join(dest_dir, f\"{random_string}.jpg\")\n",
        "                    new_LabelName = os.path.join(dest_dir, f\"{random_string}.txt\")\n",
        "                    cropped_image.save(new_ImageName, format='JPEG', quality=100)\n",
        "                    open(new_LabelName, \"w\", encoding=\"utf-8\").write(label)\n",
        "\n",
        "# 1 (1-bit pixels, black and white, stored with one pixel per byte)\n",
        "# L (8-bit pixels, grayscale)\n",
        "# RGB (3x8-bit pixels, true color)\n",
        "# RGBA (4x8-bit pixels, true color with transparency mask)"
      ],
      "metadata": {
        "id": "LkjFPSAyCSVK"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def levenshtein_distance(str1, str2):\n",
        "    len_str1 = len(str1) + 1\n",
        "    len_str2 = len(str2) + 1\n",
        "\n",
        "    # Initialize a matrix to store distances\n",
        "    matrix = [[0] * len_str2 for _ in range(len_str1)]\n",
        "\n",
        "    # Initialize the first row and column\n",
        "    for i in range(len_str1):\n",
        "        matrix[i][0] = i\n",
        "    for j in range(len_str2):\n",
        "        matrix[0][j] = j\n",
        "\n",
        "    # Calculate distances\n",
        "    for i in range(1, len_str1):\n",
        "        for j in range(1, len_str2):\n",
        "            cost = 0 if str1[i - 1] == str2[j - 1] else 1\n",
        "            matrix[i][j] = min(\n",
        "                matrix[i - 1][j] + 1,  # Deletion\n",
        "                matrix[i][j - 1] + 1,  # Insertion\n",
        "                matrix[i - 1][j - 1] + cost,  # Substitution\n",
        "            )\n",
        "\n",
        "    return matrix[len_str1 - 1][len_str2 - 1]"
      ],
      "metadata": {
        "id": "P-H34gs4CVWc"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Dataset"
      ],
      "metadata": {
        "id": "u-uI_SZNCbbl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "from torch.utils.data import Dataset\n",
        "from PIL import Image\n",
        "class SharadaDataset(Dataset):\n",
        "    \"\"\"Scripture dataset Class.\"\"\"\n",
        "\n",
        "    def __init__(self, txt_dir, img_dir, transform=None, char_dict=None):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            txt_dir (string): Path to the txt file with labels.\n",
        "            img_dir (string): Directory with all the images.\n",
        "            transform (callable, optional): Optional transform to be applied\n",
        "                on a sample.\n",
        "        \"\"\"\n",
        "        self.txt_dir = txt_dir\n",
        "        self.img_dir = img_dir\n",
        "        self.transform = transform\n",
        "        self.max_len = 0\n",
        "        self.char_list = \" -ँंःअआइईउऊऋऌऍऎएऐऑऒओऔकखगघङचछजझञटठडढणतथदधनऩपफबभमयरऱलळऴवशषसह़ऽािीुूृॄॅेैॉॊोौ्ॐ॒॑॓॔क़ख़ग़ज़ड़ढ़फ़य़ॠॢ।॥०१२३४५६७८९॰ॱॲॻॼॽॾ≈–|\"\n",
        "        if self.char_list is not None:\n",
        "            chars = sorted(list(set(self.char_list)))\n",
        "            self.char_dict = {c:i for i,c in enumerate(chars,1)}\n",
        "\n",
        "        txt_files = os.listdir(self.txt_dir)\n",
        "        self.txt_paths = [txt_file for txt_file in txt_files if txt_file.endswith('.txt')]\n",
        "        img_files = os.listdir(self.img_dir)\n",
        "        self.img_paths = [img_file for img_file in img_files if img_file.endswith('.jpg')]\n",
        "\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.txt_paths)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_name = self.img_paths[idx]\n",
        "        img_filepath = os.path.join(self.img_dir,img_name)\n",
        "        try:\n",
        "            image = Image.open(img_filepath)\n",
        "\n",
        "        except OSError:\n",
        "            image = np.random.randint(0, 255, size=(50, 100), dtype=np.uint8)\n",
        "\n",
        "        txt_name = self.txt_paths[idx]\n",
        "        txt_filepath = os.path.join(self.txt_dir,txt_name)\n",
        "        try:\n",
        "            with open(txt_filepath,'r') as file:\n",
        "                label = file.read()\n",
        "\n",
        "        except OSError:\n",
        "\n",
        "            label = \"\"\n",
        "        if len(label) > self.max_len:\n",
        "            self.max_len = len(label)\n",
        "\n",
        "        sample = {'image': image, 'label': label}\n",
        "        # print(sample)\n",
        "        if self.transform:\n",
        "            sample = self.transform(sample)\n",
        "        return sample"
      ],
      "metadata": {
        "id": "WGH20dNeCaEZ"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Dataloader"
      ],
      "metadata": {
        "id": "qyF41ZZ4ZafY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataloader Class\n",
        "import torch\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.utils.data.sampler import SubsetRandomSampler\n",
        "import numpy as np\n",
        "# from SharadaDS import SharadaDataset\n",
        "\n",
        "class SharadaDataLoader(object):\n",
        "\n",
        "    def __init__(self, ds, batch_size=(16, 16), validation_split=0.2,\n",
        "                 shuffle=True, seed=42, device='cpu'):\n",
        "        assert isinstance(ds, SharadaDataset)\n",
        "        assert isinstance(batch_size, tuple)\n",
        "        assert isinstance(validation_split, float)\n",
        "        assert isinstance(shuffle, bool)\n",
        "        assert isinstance(seed, int)\n",
        "        assert isinstance(device, str)\n",
        "\n",
        "        self.ds = ds\n",
        "        self.batch_size = batch_size\n",
        "        self.validation_split = validation_split\n",
        "        self.shuffle = shuffle\n",
        "        self.seed = seed\n",
        "        self.device = device\n",
        "\n",
        "    def  __call__(self):\n",
        "\n",
        "        dataset_size = len(self.ds)\n",
        "        indices = list(range(dataset_size))\n",
        "        split = int(np.floor(self.validation_split * dataset_size))\n",
        "\n",
        "        if self.shuffle:\n",
        "            np.random.seed(self.seed)\n",
        "            np.random.shuffle(indices)\n",
        "        train_indices, val_indices = indices[split:], indices[:split]\n",
        "\n",
        "        # Creating PT data samplers and loaders:\n",
        "        train_sampler = SubsetRandomSampler(train_indices)\n",
        "        valid_sampler = SubsetRandomSampler(val_indices)\n",
        "\n",
        "        # Dataloader\n",
        "        train_loader = DataLoader(self.ds, batch_size=self.batch_size[0],\n",
        "                                  sampler=train_sampler, collate_fn=self.collate_fn)\n",
        "        validation_loader = DataLoader(self.ds, batch_size=self.batch_size[1],\n",
        "                                       sampler=valid_sampler, collate_fn=self.collate_fn)\n",
        "\n",
        "        return train_loader, validation_loader\n",
        "\n",
        "\n",
        "\n",
        "    def collate_fn(self, batch):\n",
        "        \"\"\"Creates mini-batch tensors from the list of tuples (image, label).\n",
        "\n",
        "        We should build custom collate_fn rather than using default collate_fn,\n",
        "        because merging label tensor creates jagged array.\n",
        "        Args:\n",
        "            data: list of tuple (image, caption).\n",
        "                - image: torch tensor of shape (1, 128, 32).\n",
        "                - label: torch tensor of shape (?); variable length.\n",
        "        Returns:\n",
        "            images: torch tensor of shape (batch_size, chan_in, height, width).\n",
        "            targets: torch tensor of shape (sum(target_lengths)).\n",
        "            lengths: torch tensor; length of each target label.\n",
        "        \"\"\"\n",
        "\n",
        "        # Sort a data list by caption length (descending order).\n",
        "        #sample.sort(key=lambda x: len(x[1]), reverse=True)\n",
        "        images, labels = [b.get('image') for b in batch], [b.get('label') for b in batch]\n",
        "\n",
        "        # Merge images (from tuple of 3D tensor to 4D tensor).\n",
        "        images = torch.stack(images, 0)\n",
        "\n",
        "        # Merge captions (from tuple of 1D tensor to 2D tensor).\n",
        "        lengths = [len(label) for label in labels]\n",
        "        targets = torch.zeros(sum(lengths)).long()\n",
        "        lengths = torch.tensor(lengths)\n",
        "        for j, label in enumerate(labels):\n",
        "            start = sum(lengths[:j])\n",
        "            end = lengths[j]\n",
        "            targets[start:start+end] = torch.tensor([self.ds.char_dict.get(letter) for letter in label]).long()\n",
        "\n",
        "        if self.device == 'cpu':\n",
        "            dev = torch.device('cpu')\n",
        "        else:\n",
        "            dev = torch.device('cuda')\n",
        "\n",
        "        return images.to(dev), targets.to(dev), lengths.to(dev)\n"
      ],
      "metadata": {
        "id": "zYPTgSPfZZNu"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Model"
      ],
      "metadata": {
        "id": "9dwSjk8li68D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "from torch.nn import functional as F\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "class ResidualBlock(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, stride=1):\n",
        "        super(ResidualBlock, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=stride, padding=1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=1, padding=1, bias=False)\n",
        "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
        "        self.downsample = nn.Sequential()\n",
        "        if stride != 1 or in_channels != out_channels:\n",
        "            self.downsample = nn.Sequential(\n",
        "                nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=stride, bias=False),\n",
        "                nn.BatchNorm2d(out_channels)\n",
        "            )\n",
        "\n",
        "    def forward(self, x):\n",
        "        identity = x\n",
        "        out = self.conv1(x)\n",
        "        out = self.bn1(out)\n",
        "        out = self.relu(out)\n",
        "        out = self.conv2(out)\n",
        "        out = self.bn2(out)\n",
        "        out += self.downsample(identity)\n",
        "        out = self.relu(out)\n",
        "        return out\n",
        "\n",
        "class CNN(nn.Module):\n",
        "    def __init__(self, input_channels, hidden_size):\n",
        "        super(CNN, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(input_channels, 64, kernel_size=3, stride=1, padding=1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(64)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.resblocks = nn.Sequential(\n",
        "            ResidualBlock(64, 64),\n",
        "            ResidualBlock(64, 128, stride=2),\n",
        "            ResidualBlock(128, 256, stride=2)\n",
        "        )\n",
        "        self.conv2 = nn.Conv2d(256, hidden_size, kernel_size=1, stride=1, padding=0, bias=False)\n",
        "        self.pool = nn.AdaptiveAvgPool2d((1, None))\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv1(x)\n",
        "        x = self.bn1(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.resblocks(x)\n",
        "        x = self.conv2(x)\n",
        "        x = self.pool(x)\n",
        "        return x\n",
        "\n",
        "class BiLSTM(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, num_layers, num_classes):\n",
        "        super(BiLSTM, self).__init__()\n",
        "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, bidirectional=True, batch_first=True)\n",
        "        self.fc = nn.Linear(hidden_size * 2, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out, _ = self.lstm(x)\n",
        "        T, b, h = out.size() # T - time_steps\n",
        "        # print(out.size())\n",
        "        out = self.fc(out[:, -1, :])\n",
        "        return out\n",
        "\n",
        "class CRNN(nn.Module):\n",
        "    def __init__(self, input_channels, hidden_size, num_layers, num_classes):\n",
        "        super(CRNN, self).__init__()\n",
        "        self.cnn = CNN(input_channels, hidden_size)\n",
        "        self.rnn = BiLSTM(hidden_size, hidden_size, num_layers, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.cnn(x)\n",
        "        # print(\"shape\",x.shape)\n",
        "        x = x.squeeze(2)\n",
        "        # print(\"after squeeze\", x.shape )\n",
        "        x = x.permute(2, 0, 1)\n",
        "        x = self.rnn(x)\n",
        "        print(x.shape)\n",
        "        return x"
      ],
      "metadata": {
        "id": "gPO3YjsUD_kz"
      },
      "execution_count": 109,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Transforms"
      ],
      "metadata": {
        "id": "rmRjR_rYZlha"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# import torch\n",
        "# from torch import nn, optim\n",
        "# import numpy as np\n",
        "# import matplotlib.pyplot as plt\n",
        "# import os\n",
        "# import pickle\n",
        "# import Levenshtein as leven\n",
        "# from skimage.color import rgb2gray\n",
        "# from skimage.transform import rotate\n",
        "# import matplotlib.pyplot as plt\n",
        "\n"
      ],
      "metadata": {
        "id": "7g5my4idZjFn"
      },
      "execution_count": 110,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# transforms\n",
        "# Transform and Data Augmentation\n",
        "from skimage import transform, color, filters\n",
        "import cv2 as cv\n",
        "import numpy as np\n",
        "import torch\n",
        "from torchvision.transforms import Normalize\n",
        "import torchvision.transforms.functional as F\n",
        "\n",
        "class PadResize(object):\n",
        "    def __init__(self, output_size):\n",
        "        assert isinstance(output_size, (tuple))\n",
        "        self.output_size = output_size\n",
        "\n",
        "    def __call__(self, sample):\n",
        "        image, label = sample['image'], sample['label']\n",
        "        self.w_f, self.h_f = self.output_size\n",
        "        ratio_final = self.w_f / self.h_f\n",
        "        self.w, self.h = image.size\n",
        "        self.ratio_current = self.w / self.h\n",
        "\n",
        "        # check if the original and final aspect ratios are the same within a margin\n",
        "        if round(self.ratio_current, 2) != round(ratio_final, 2):\n",
        "            # padding to preserve aspect ratio\n",
        "            hp = int(self.w/ratio_final - self.h)\n",
        "            wp = int(ratio_final * self.h - self.w)\n",
        "\n",
        "            if hp > 0 and wp < 0:\n",
        "                hp = hp // 2\n",
        "                image = F.pad(image, (0, hp, 0, hp), 0, \"constant\")\n",
        "                image = F.resize(image, [self.h_f, self.w_f])\n",
        "            elif wp > 0 and hp < 0:\n",
        "                wp = wp // 2\n",
        "                image = F.pad(image, (wp, 0, wp, 0), 0, \"constant\")\n",
        "                image = F.resize(image, [self.h_f, self.w_f])\n",
        "        else:\n",
        "            image = F.resize(image,[self.h_f, self.w_f])\n",
        "\n",
        "        return {'image': image, 'label': label}\n",
        "\n",
        "class Deskew(object):\n",
        "    \"\"\"Deskew handwriting samples\"\"\"\n",
        "\n",
        "    def __call__(self, sample):\n",
        "        image, label = sample['image'], sample['label']\n",
        "\n",
        "        try:\n",
        "            threshold = filters.threshold_otsu(image)\n",
        "        except ValueError:\n",
        "            return {'image':image, 'label':label}\n",
        "\n",
        "        binary = image.copy() < threshold\n",
        "\n",
        "        # array of alpha values\n",
        "        alphas = np.arange(-1, 1.1, 0.25)\n",
        "        alpha_res = np.array([])\n",
        "        alpha_params = []\n",
        "\n",
        "        for a in alphas:\n",
        "            alpha_sum = 0\n",
        "            shift_x = np.max([-a*binary.shape[0], 0])\n",
        "            M = np.array([[1, a, shift_x],\n",
        "                          [0,1,0]], dtype=np.float64)\n",
        "            img_size = (np.int(binary.shape[1] + np.ceil(np.abs(a*binary.shape[0]))), binary.shape[0])\n",
        "            alpha_params.append((M, img_size))\n",
        "\n",
        "\n",
        "            img_shear = cv.warpAffine(src=binary.astype(np.uint8),\n",
        "                                      M=M, dsize=img_size,\n",
        "                                      flags=cv.INTER_NEAREST)\n",
        "\n",
        "            for i in range(0, img_shear.shape[1]):\n",
        "                if not np.any(img_shear[:, i]):\n",
        "                    continue\n",
        "\n",
        "                h_alpha = np.sum(img_shear[:, i])\n",
        "                fgr_pos = np.where(img_shear[:, i] == 1)\n",
        "                delta_y_alpha = fgr_pos[0][-1] - fgr_pos[0][0] + 1\n",
        "\n",
        "                if h_alpha == delta_y_alpha:\n",
        "                    alpha_sum += h_alpha**2\n",
        "\n",
        "            alpha_res = np.append(alpha_res, alpha_sum)\n",
        "\n",
        "        best_M, best_size = alpha_params[alpha_res.argmax()]\n",
        "        deskewed_img = cv.warpAffine(src=image, M=best_M, dsize=best_size,\n",
        "                                      flags=cv.INTER_LINEAR,\n",
        "                                      borderMode=cv.BORDER_CONSTANT,\n",
        "                                      borderValue=255)\n",
        "\n",
        "        return {'image':deskewed_img, 'label':label}\n",
        "\n",
        "class toRGB(object):\n",
        "    \"\"\"Convert the ndarrys to RGB tensors.\n",
        "       Required if using ImageNet pretrained Resnet.\"\"\"\n",
        "\n",
        "    def __call__(self, sample):\n",
        "        image, label = sample['image'], sample['label']\n",
        "        image = color.gray2rgb(image)\n",
        "\n",
        "        return {'image': image, 'label': label}\n",
        "\n",
        "class ToTensor(object):\n",
        "    \"\"\"Convert ndarrays in sample to Tensors.\"\"\"\n",
        "\n",
        "    def __init__(self, rgb=True):\n",
        "        assert isinstance(rgb, bool)\n",
        "        self.rgb = rgb\n",
        "\n",
        "    def __call__(self, sample):\n",
        "        image, label = sample['image'], sample['label']\n",
        "\n",
        "        # swap color axis because\n",
        "        # numpy image: H x W x C\n",
        "        # torch image: C X H X W\n",
        "        image = F.to_tensor(image)\n",
        "        return {'image': image, 'label': label}\n",
        "\n",
        "\n",
        "\n",
        "class Normalize_Cust(object):\n",
        "    \"\"\"Normalise by channel mean and std\"\"\"\n",
        "\n",
        "    def __init__(self, mean, std):\n",
        "        self.mean = torch.tensor(mean, dtype=torch.float)\n",
        "        self.std = torch.tensor(std, dtype=torch.float)\n",
        "        self.norm = Normalize(mean, std)\n",
        "\n",
        "    def __call__(self, sample):\n",
        "        image, label = sample['image'], sample['label']\n",
        "        return {'image': self.norm(image), 'label': label}"
      ],
      "metadata": {
        "id": "qLkXastgZpnv"
      },
      "execution_count": 111,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Train"
      ],
      "metadata": {
        "id": "POxmAQzAlYT6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch import nn, optim\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "import pickle\n",
        "from skimage.color import rgb2gray\n",
        "from skimage.transform import rotate\n",
        "import matplotlib.pyplot as plt\n",
        "from torch.optim import Adam\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "from torchvision.transforms import Compose\n",
        "from torchvision import transforms\n",
        "\n",
        "# from utils import *\n",
        "# from dataset import SharadaDataset\n",
        "# from dataloader import SharadaDataLoader\n",
        "# from transforms import PadResize, Deskew, toRGB, ToTensor, Normalize_Cust\n",
        "\n",
        "os.makedirs(\"chk_pts/\", exist_ok=True)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(device)\n",
        "\n",
        "\n",
        "\n",
        "dataset = SharadaDataset(txt_dir='/content/extracted_dir/',\n",
        "                        img_dir='/content/extracted_dir/',\n",
        "                        transform=Compose([\n",
        "                            # Deslant(),\n",
        "                            PadResize(output_size=(64,200)),\n",
        "                            ToTensor(), # converted to Tensor\n",
        "                            Normalize_Cust(mean=[0.485, 0.456, 0.406],std=[0.229, 0.224, 0.225])\n",
        "                        ]))\n",
        "\n",
        "# dataset = DevDataset(images,\n",
        "#                         labels,\n",
        "#                         transform=Compose([\n",
        "#                             # Deslant(),\n",
        "#                             PadResize(output_size=(64,200)),\n",
        "#                             ToTensor(), # converted to Tensor\n",
        "#                             Normalize_Cust(mean=[0.485, 0.456, 0.406],std=[0.229, 0.224, 0.225])\n",
        "#                         ]))\n",
        "\n",
        "dl = SharadaDataLoader(dataset,\n",
        "                       batch_size=(120,240),\n",
        "                       validation_split=0.2,\n",
        "                       shuffle=True,\n",
        "                       seed=3407,\n",
        "                       device=str(device))\n",
        "\n",
        "crnn_model = CRNN(input_channels=3, hidden_size=512, num_layers=2, num_classes=len(dataset.char_dict) + 1).to(device)\n",
        "optimizer = Adam(crnn_model.parameters(), lr=0.001)\n",
        "\n",
        "ctc_loss = nn.CTCLoss(blank=0, reduction='mean')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cQ5r-SKdCuN-",
        "outputId": "713c477a-bd80-43f1-936a-8ba48d73ae12"
      },
      "execution_count": 112,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(crnn_model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z2xZkldGo_ex",
        "outputId": "88ed1036-5260-45f1-b5c0-f4c4de952723"
      },
      "execution_count": 113,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CRNN(\n",
            "  (cnn): CNN(\n",
            "    (conv1): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "    (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (relu): ReLU(inplace=True)\n",
            "    (resblocks): Sequential(\n",
            "      (0): ResidualBlock(\n",
            "        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu): ReLU(inplace=True)\n",
            "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (downsample): Sequential()\n",
            "      )\n",
            "      (1): ResidualBlock(\n",
            "        (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu): ReLU(inplace=True)\n",
            "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (downsample): Sequential(\n",
            "          (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
            "          (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        )\n",
            "      )\n",
            "      (2): ResidualBlock(\n",
            "        (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu): ReLU(inplace=True)\n",
            "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (downsample): Sequential(\n",
            "          (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
            "          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "    (conv2): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "    (pool): AdaptiveAvgPool2d(output_size=(1, None))\n",
            "  )\n",
            "  (rnn): BiLSTM(\n",
            "    (lstm): LSTM(512, 512, num_layers=2, batch_first=True, bidirectional=True)\n",
            "    (fc): Linear(in_features=1024, out_features=113, bias=True)\n",
            "  )\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torchinfo import summary\n",
        "summary(crnn_model, (100, 3, 64, 64))"
      ],
      "metadata": {
        "id": "5qT7WLmQsOiU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cfe78f48-7754-4844-aa6d-7d5e7a873635"
      },
      "execution_count": 114,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([16, 113])\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "===============================================================================================\n",
              "Layer (type:depth-idx)                        Output Shape              Param #\n",
              "===============================================================================================\n",
              "CRNN                                          [16, 113]                 --\n",
              "├─CNN: 1-1                                    [100, 512, 1, 16]         --\n",
              "│    └─Conv2d: 2-1                            [100, 64, 64, 64]         1,728\n",
              "│    └─BatchNorm2d: 2-2                       [100, 64, 64, 64]         128\n",
              "│    └─ReLU: 2-3                              [100, 64, 64, 64]         --\n",
              "│    └─Sequential: 2-4                        [100, 256, 16, 16]        --\n",
              "│    │    └─ResidualBlock: 3-1                [100, 64, 64, 64]         73,984\n",
              "│    │    └─ResidualBlock: 3-2                [100, 128, 32, 32]        230,144\n",
              "│    │    └─ResidualBlock: 3-3                [100, 256, 16, 16]        919,040\n",
              "│    └─Conv2d: 2-5                            [100, 512, 16, 16]        131,072\n",
              "│    └─AdaptiveAvgPool2d: 2-6                 [100, 512, 1, 16]         --\n",
              "├─BiLSTM: 1-2                                 [16, 113]                 --\n",
              "│    └─LSTM: 2-7                              [16, 100, 1024]           10,502,144\n",
              "│    └─Linear: 2-8                            [16, 113]                 115,825\n",
              "===============================================================================================\n",
              "Total params: 11,974,065\n",
              "Trainable params: 11,974,065\n",
              "Non-trainable params: 0\n",
              "Total mult-adds (G): 98.04\n",
              "===============================================================================================\n",
              "Input size (MB): 4.92\n",
              "Forward/backward pass size (MB): 2319.99\n",
              "Params size (MB): 47.90\n",
              "Estimated Total Size (MB): 2372.80\n",
              "==============================================================================================="
            ]
          },
          "metadata": {},
          "execution_count": 114
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_loader, val_loader = dl()"
      ],
      "metadata": {
        "id": "q81m-2lFC-e3"
      },
      "execution_count": 98,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "writer = SummaryWriter()\n",
        "num_epochs = 10\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(num_epochs):\n",
        "    crnn_model.train()  # Set the model to training mode\n",
        "    total_loss = 0.0\n",
        "\n",
        "    # Iterate over the training dataset\n",
        "    for images, targets, lengths in train_loader:  # Assuming dl() returns train_loader\n",
        "        # print(\"Here:\",images)\n",
        "        images = images.to(device)\n",
        "        targets = targets.to(device)\n",
        "\n",
        "        # Forward pass\n",
        "        logits = crnn_model(images)\n",
        "        print(logits.shape)\n",
        "\n",
        "        # Calculate the CTC loss\n",
        "        loss = ctc_loss(logits, targets, lengths, lengths)\n",
        "\n",
        "        # Backward pass and optimization\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    # Calculate average training loss for the epoch\n",
        "    avg_loss = total_loss / len(train_loader)\n",
        "\n",
        "    # Log the training loss to Tensorboard\n",
        "    writer.add_scalar('Loss/Train', avg_loss, epoch)\n",
        "\n",
        "    print(f'Epoch [{epoch + 1}/{num_epochs}], Loss: {avg_loss:.4f}')\n",
        "\n",
        "    # Validation\n",
        "    if (epoch + 1) % 1 == 0:  # You can adjust the frequency of validation\n",
        "        crnn_model.eval()  # Set the model to evaluation mode\n",
        "        val_loss = 0.0\n",
        "\n",
        "        # Iterate over the validation dataset\n",
        "        with torch.no_grad():\n",
        "            for val_images, val_targets, val_lengths in val_loader:  # Assuming dl() returns validation_loader\n",
        "                val_images = val_images.to(device)\n",
        "                val_targets = val_targets.to(device)\n",
        "\n",
        "                # Forward pass\n",
        "                val_logits = crnn_model(val_images)\n",
        "\n",
        "                # Calculate the CTC loss\n",
        "                val_loss += ctc_loss(val_logits, val_targets, val_lengths, val_lengths).item()\n",
        "\n",
        "                _, predicted_labels = torch.max(val_logits, 2)\n",
        "                predicted_labels = [\"\".join([dataset.char_list[c] for c in row if c != 0]) for row in predicted_labels.cpu().numpy()]\n",
        "\n",
        "                for pred, target in zip(predicted_labels, val_targets.cpu().numpy()):\n",
        "                    distance = levenshtein_distance(pred, \"\".join([dataset.char_list[c] for c in target if c != 0]))\n",
        "\n",
        "                    writer.add_scalar('LevenshteinDistance/Validation', distance, epoch)\n",
        "\n",
        "        avg_val_loss = val_loss / len(val_loader)\n",
        "        writer.add_scalar('Loss/Validation', avg_val_loss, epoch)\n",
        "        crnn_model.train()\n",
        "\n",
        "        print(f'Validation Loss: {avg_val_loss:.4f}')\n",
        "\n",
        "# Save the trained model\n",
        "torch.save(crnn_model.state_dict(), 'chk_pts/crnn_model.pth')\n",
        "\n",
        "# Close Tensorboard writer\n",
        "writer.close()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 461
        },
        "id": "bP44MsXpDG7X",
        "outputId": "4583a005-db73-4cf1-eb1d-377c761bc856"
      },
      "execution_count": 102,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "shape torch.Size([120, 512, 1, 16])\n",
            "after squeeze torch.Size([120, 512, 16])\n",
            "torch.Size([16, 120, 1024])\n",
            "torch.Size([16, 113])\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "input_lengths must be of size batch_size",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-102-0a4aa8ee7d28>\u001b[0m in \u001b[0;36m<cell line: 5>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0;31m# Calculate the CTC loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mctc_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlengths\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlengths\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0;31m# Backward pass and optimization\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1517\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1518\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1519\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1520\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1525\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1526\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1528\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1529\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/loss.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, log_probs, targets, input_lengths, target_lengths)\u001b[0m\n\u001b[1;32m   1768\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1769\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlog_probs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_lengths\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_lengths\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1770\u001b[0;31m         return F.ctc_loss(log_probs, targets, input_lengths, target_lengths, self.blank, self.reduction,\n\u001b[0m\u001b[1;32m   1771\u001b[0m                           self.zero_infinity)\n\u001b[1;32m   1772\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mctc_loss\u001b[0;34m(log_probs, targets, input_lengths, target_lengths, blank, reduction, zero_infinity)\u001b[0m\n\u001b[1;32m   2654\u001b[0m             \u001b[0mblank\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mblank\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduction\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreduction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mzero_infinity\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mzero_infinity\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2655\u001b[0m         )\n\u001b[0;32m-> 2656\u001b[0;31m     return torch.ctc_loss(\n\u001b[0m\u001b[1;32m   2657\u001b[0m         \u001b[0mlog_probs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_lengths\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_lengths\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mblank\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_enum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreduction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mzero_infinity\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2658\u001b[0m     )\n",
            "\u001b[0;31mRuntimeError\u001b[0m: input_lengths must be of size batch_size"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Other Model Sample"
      ],
      "metadata": {
        "id": "2qPRmXLFpXPN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class IEncoder(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(IEncoder, self).__init__()\n",
        "\n",
        "        self.encoder = nn.Sequential(\n",
        "            nn.Conv2d(3, 16, kernel_size=3, stride=2, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(16, 32, kernel_size=3, stride=2, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(32, 64, kernel_size=3, stride=2, padding=1),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.encoder(x)\n",
        "\n",
        "class TEncoder(nn.Module):\n",
        "    def __init__(self, input_size, latent_size):\n",
        "        super(TEncoder, self).__init__()\n",
        "\n",
        "        self.encoder = nn.LSTM(input_size, latent_size, batch_first=True)\n",
        "\n",
        "    def forward(self, x):\n",
        "        _, (hidden, _) = self.encoder(x)\n",
        "        return hidden.squeeze(0)\n",
        "\n",
        "class LatentProjectionSpace(nn.Module):\n",
        "    def __init__(self, latent_size):\n",
        "        super(LatentProjectionSpace, self).__init__()\n",
        "\n",
        "        self.projection = nn.Linear(64 + latent_size, latent_size)\n",
        "\n",
        "    def forward(self, img_emb, txt_emb):\n",
        "        combined_latent = torch.cat((img_emb.view(img_emb.size(0), -1), txt_emb), dim=1)\n",
        "        return self.projection(combined_latent)\n",
        "\n",
        "class IDecoder(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(IDecoder, self).__init__()\n",
        "\n",
        "        self.decoder = nn.Sequential(\n",
        "            nn.ConvTranspose2d(64, 32, kernel_size=3, stride=2, padding=1, output_padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.ConvTranspose2d(32, 16, kernel_size=3, stride=2, padding=1, output_padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.ConvTranspose2d(16, 3, kernel_size=3, stride=2, padding=1, output_padding=1),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.decoder(x)\n",
        "\n",
        "class TDecoder(nn.Module):\n",
        "    def __init__(self, latent_size, output_size):\n",
        "        super(TDecoder, self).__init__()\n",
        "\n",
        "        self.decoder = nn.LSTM(latent_size, output_size, batch_first=True)\n",
        "\n",
        "    def forward(self, x):\n",
        "        output, _ = self.decoder(x.unsqueeze(1))\n",
        "        return output.squeeze(1)\n",
        "\n",
        "class ImgTextAutoencoder(nn.Module):\n",
        "    def __init__(self, img_input_size, txt_input_size, latent_size):\n",
        "        super(ImgTextAutoencoder, self).__init__()\n",
        "\n",
        "        self.image_encoder = IEncoder()\n",
        "        self.text_encoder = TEncoder(txt_input_size, latent_size)\n",
        "        self.latent_projection = LatentProjectionSpace(latent_size)\n",
        "        self.image_decoder = IDecoder()\n",
        "        self.text_decoder = TDecoder(latent_size, txt_input_size)\n",
        "\n",
        "    def forward(self, img_input, txt_input):\n",
        "        img_emb = self.image_encoder(img_input)\n",
        "        txt_emb = self.text_encoder(txt_input)\n",
        "        latent = self.latent_projection(img_emb, txt_emb)\n",
        "        rec_img = self.image_decoder(img_emb)\n",
        "        rec_txt = self.text_decoder(latent)\n",
        "        return rec_img, rec_txt\n",
        "\n",
        "img_input_size = (3, 64, 64)\n",
        "txt_input_size = 300\n",
        "latent_size = 100\n",
        "\n",
        "img_text_autoencoder = ImgTextAutoencoder(img_input_size, txt_input_size, latent_size)\n",
        "\n",
        "print(img_text_autoencoder)\n"
      ],
      "metadata": {
        "id": "x3RB0iVbiqrJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "USl18pO5qKDd"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}